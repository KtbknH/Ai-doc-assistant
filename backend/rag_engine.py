import os
from anthropic import Anthropic
import chromadb


class RAGEngine:
    """
    Moteur RAG (Retrieval-Augmented Generation) avec Claude.
    
    Permet de:
    - Ajouter des documents √† une base vectorielle
    - Rechercher des passages pertinents
    - G√©n√©rer des r√©ponses avec ou sans contexte RAG
    """
    
    def __init__(self):
        """Initialise le client Claude et la base ChromaDB."""
    
        api_key = os.getenv("ANTHROPIC_API_KEY")
        if not api_key:
            raise ValueError("‚ùå ANTHROPIC_API_KEY non trouv√©e dans .env")
    
        self.client = Anthropic(api_key=api_key)
        
        self.chroma_client = chromadb.Client()
  
        self.collection = self.chroma_client.get_or_create_collection(
            name="documents"
        )
        
        print("‚úÖ RAG Engine initialis√© avec Claude")
    
    def add_document(self, text: str, doc_id: str) -> int:
        """
        Ajoute un document √† la base vectorielle.
        
        Le texte est d√©coup√© en chunks de 500 caract√®res
        pour optimiser la recherche s√©mantique.
        
        Args:
            text: Contenu du document
            doc_id: Identifiant unique du document
            
        Returns:
            Nombre de chunks cr√©√©s
        """
        chunk_size = 300
        overlap = 100 
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + chunk_size
            chunk = text[start:end]
            
            if chunk.strip():
                chunks.append(chunk)
            
            start = end - overlap
        
        for i, chunk in enumerate(chunks):
            chunk_id = f"{doc_id}_chunk_{i}"
            
            existing = self.collection.get(ids=[chunk_id])
            if not existing['ids']:
                self.collection.add(
                    documents=[chunk],
                    ids=[chunk_id],
                    metadatas=[{
                        "source": doc_id,
                        "chunk_index": i,
                        "total_chunks": len(chunks)
                    }]
                )
        
        print(f"‚úÖ {len(chunks)} chunks ajout√©s pour '{doc_id}'")
        return len(chunks)
    
    def search_context(self, query: str, n_results: int = 3) -> list:
        """
        Recherche les passages les plus pertinents pour une question.
        
        Utilise la recherche s√©mantique de ChromaDB pour trouver
        les chunks les plus similaires √† la question.
        
        Args:
            query: Question de l'utilisateur
            n_results: Nombre de r√©sultats √† retourner (d√©faut: 3)
            
        Returns:
            Liste des chunks pertinents
        """

        if self.collection.count() == 0:
            print("‚ö†Ô∏è Aucun document dans la base")
            return []
        
        results = self.collection.query(
            query_texts=[query],
            n_results=min(n_results, self.collection.count())
        )
    
        if results and results['documents']:
            chunks = results['documents'][0]
            print(f"üîç {len(chunks)} chunk(s) trouv√©(s)")
            return chunks
        
        return []
    
    def generate_answer(self, query: str, use_rag: bool = True) -> dict:
        """
        G√©n√®re une r√©ponse avec Claude, avec ou sans RAG.
        
        Mode RAG activ√©:
        1. Recherche le contexte pertinent dans les documents
        2. Injecte le contexte dans le prompt
        3. Claude r√©pond en se basant sur le contexte
        
        Mode RAG d√©sactiv√©:
        - Claude r√©pond directement sans contexte
        
        Args:
            query: Question de l'utilisateur
            use_rag: Activer le mode RAG (d√©faut: True)
            
        Returns:
            Dict avec answer, sources, mode, model
        """
        context_chunks = []
        
        if use_rag:

            context_chunks = self.search_context(query, n_results=5)
            context_str = "\n\n---\n\n".join(context_chunks)
        
            prompt = f"""Tu es un assistant personnel intelligent. Tu dois r√©pondre √† la question en te basant sur le contexte fourni ci-dessous.

<contexte>
{context_str}
</contexte>

<r√®gles>
- Utilise TOUTES les informations pertinentes du contexte pour construire une r√©ponse compl√®te
- Pour les questions sur une personne (ex: "Qui est X?", "Parle-moi de X"), pr√©sente un profil complet avec: formation, comp√©tences, exp√©rience, contact si disponibles
- Structure ta r√©ponse de mani√®re claire et lisible
- Si l'information n'est pas dans le contexte, dis "Je ne trouve pas cette information dans les documents"
- Sois naturel et conversationnel dans tes r√©ponses
</r√®gles>

<question>
{query}
</question>

R√©ponse:"""
        else:
           
            prompt = query
        
        # 3. Appeler Claude Haiku (le moins cher)
        try:
            response = self.client.messages.create(
                model="claude-3-haiku-20240307",
                max_tokens=1024,
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )
            
            answer = response.content[0].text
            
        except Exception as e:
            print(f"‚ùå Erreur Claude: {e}")
            answer = f"Erreur lors de la g√©n√©ration: {str(e)}"
        
        return {
            "answer": answer,
            "sources": context_chunks,
            "mode": "RAG" if use_rag else "Direct",
            "model": "claude-3-haiku-20240307",
            "context_used": len(context_chunks) > 0
        }
    
    def get_stats(self) -> dict:
        """Retourne les statistiques de la base de documents."""
        return {
            "total_chunks": self.collection.count(),
            "model": "claude-3-haiku-20240307"
        }


# =============================================================================
# TEST LOCAL
# =============================================================================
if __name__ == "__main__":
    from dotenv import load_dotenv
    
    # Charger les variables d'environnement
    load_dotenv()
    
    print("=" * 50)
    print("üß™ Test du RAG Engine avec Claude")
    print("=" * 50)
    
    rag = RAGEngine()
    
    doc_python = """
    Python est un langage de programmation cr√©√© par Guido van Rossum en 1991.
    Il est caract√©ris√© par sa syntaxe claire et lisible.
    Python est tr√®s populaire en data science, machine learning et d√©veloppement web.
    Les frameworks populaires incluent Django, Flask et FastAPI.
    Python utilise l'indentation pour d√©finir les blocs de code.
    """
    
    doc_claude = """
    Claude est une intelligence artificielle cr√©√©e par Anthropic.
    Anthropic a √©t√© fond√©e en 2021 par Dario Amodei et Daniela Amodei.
    Claude est con√ßu pour √™tre utile, honn√™te et inoffensif.
    Il existe plusieurs versions: Claude Haiku, Claude Sonnet et Claude Opus.
    Claude Haiku est le mod√®le le plus rapide et le moins cher.
    """
    
    doc_rag = """
    RAG signifie Retrieval-Augmented Generation.
    Cette technique combine la recherche d'information avec la g√©n√©ration de texte.
    Le RAG permet de r√©duire les hallucinations des mod√®les de langage.
    ChromaDB est une base de donn√©es vectorielle souvent utilis√©e pour le RAG.
    Les embeddings permettent de repr√©senter le texte sous forme de vecteurs.
    """
    
    rag.add_document(doc_python, "python_info")
    rag.add_document(doc_claude, "claude_info")
    rag.add_document(doc_rag, "rag_info")
    
    print("\n" + "=" * 50)
    print("üìä Statistiques:", rag.get_stats())
    print("=" * 50)
    
    # Test avec RAG
    print("\nü§ñ Test AVEC RAG:")
    print("-" * 30)
    question = "Qui a cr√©√© Python et en quelle ann√©e?"
    result = rag.generate_answer(question, use_rag=True)
    print(f"Q: {question}")
    print(f"R: {result['answer']}")
    print(f"Sources: {len(result['sources'])} chunk(s)")
    
    # Test sans RAG
    print("\nü§ñ Test SANS RAG:")
    print("-" * 30)
    result_no_rag = rag.generate_answer(question, use_rag=False)
    print(f"Q: {question}")
    print(f"R: {result_no_rag['answer']}")
    
    print("\n‚úÖ Tests termin√©s!")